#!/usr/bin/env python3
"""
Complete generator for Section 6 - C Implementation
This script creates a comprehensive Jupyter notebook with all 5 tasks (21-25)
"""

import json
import sys

def add_cell(cells, cell_type, content, execution_count=None):
    """Helper to add a cell to the notebook"""
    cell = {
        "cell_type": cell_type,
        "metadata": {},
        "source": content.split('\n') if isinstance(content, str) else content
    }
    if cell_type == "code":
        cell["execution_count"] = execution_count
        cell["outputs"] = []
    cells.append(cell)

def create_complete_section6():
    """Generate the complete Section 6 notebook"""
    
    # Load existing notebook
    with open('notebooks/seccion6.ipynb', 'r', encoding='utf-8') as f:
        notebook = json.load(f)
    
    # Clear existing cells except header
    cells = notebook["cells"][:2]  # Keep header and config
    
    # ============================================
    # TAREA 21: MARKDOWN EXPLANATION
    # ============================================
    add_cell(cells, "markdown", """---

# ============================================
# TAREA 21: Selecci√≥n y Justificaci√≥n del Algoritmo
# ============================================

## üéØ Objetivo
Seleccionar un algoritmo de aprendizaje supervisado para implementar en C y justificar t√©cnicamente la elecci√≥n.

## üìä An√°lisis de Candidatos

### Algoritmos Considerados:

#### 1. K-Nearest Neighbors (KNN) ‚≠ê SELECCIONADO
**Ventajas para implementaci√≥n en C:**
- ‚úÖ Algoritmo conceptualmente simple (b√∫squeda + votaci√≥n)
- ‚úÖ No requiere fase de entrenamiento compleja (solo almacenar datos)
- ‚úÖ F√°cil de entender y debuggear
- ‚úÖ Implementaci√≥n directa sin optimizaciones avanzadas
- ‚úÖ Estructuras de datos simples (arrays)
- ‚úÖ C√°lculos matem√°ticos b√°sicos (distancia euclidiana)

**Desventajas:**
- ‚ö†Ô∏è Complejidad O(n*d) en predicci√≥n (n=tama√±o dataset, d=dimensiones)
- ‚ö†Ô∏è Sensible a escalamiento de features
- ‚ö†Ô∏è Requiere mucha memoria para datasets grandes

#### 2. Regresi√≥n Log√≠stica
**Ventajas:**
- ‚úÖ Interpretable
- ‚úÖ R√°pida en predicci√≥n

**Desventajas:**
- ‚ùå Entrenamiento complejo (gradiente descendente, optimizaci√≥n)
- ‚ùå Requiere manejo de convergencia
- ‚ùå Multiclass (OvR o Softmax) a√±ade complejidad

#### 3. √Årbol de Decisi√≥n
**Ventajas:**
- ‚úÖ Visualizable
- ‚úÖ No requiere normalizaci√≥n

**Desventajas:**
- ‚ùå Algoritmo de construcci√≥n complejo (splits, gini, poda)
- ‚ùå Estructuras de datos complejas (√°rboles, recursi√≥n)

#### 4. Naive Bayes
**Ventajas:**
- ‚úÖ Simple probabil√≠sticamente

**Desventajas:**
- ‚ùå Requiere estimaci√≥n de distribuciones
- ‚ùå Manejo de underflow num√©rico
- ‚ùå Variables continuas requieren discretizaci√≥n

#### 5. Perceptr√≥n
**Ventajas:**
- ‚úÖ Simple conceptualmente

**Desventajas:**
- ‚ùå Solo funciona bien para datos linealmente separables
- ‚ùå Multiclass requiere estrategia adicional

---

## ‚úÖ DECISI√ìN FINAL: K-Nearest Neighbors (KNN)

### Justificaci√≥n T√©cnica:

1. **Simplicidad de Implementaci√≥n**: KNN no requiere entrenamiento complejo. Solo necesitamos:
   - Almacenar datos de entrenamiento
   - Calcular distancias euclidianas
   - Encontrar k vecinos m√°s cercanos
   - Votar por la clase mayoritaria

2. **Estructuras de Datos Simples**: Se implementa con arrays est√°ticos en C, sin necesidad de √°rboles, grafos o estructuras din√°micas complejas.

3. **Matem√°ticas Elementales**: Solo requiere:
   - Ra√≠z cuadrada (disponible en math.h)
   - Sumas y restas
   - Comparaciones

4. **Debugging Sencillo**: F√°cil de verificar paso a paso (imprimir distancias, vecinos, votos).

5. **Rendimiento Aceptable**: Para un subconjunto reducido del dataset, KNN es viable y permite demostrar comprensi√≥n algor√≠tmica.

6. **Comparaci√≥n Python vs C Significativa**: Podemos comparar directamente con sklearn.neighbors.KNeighborsClassifier

---

## üîß Configuraci√≥n Seleccionada

- **Algoritmo**: K-Nearest Neighbors (KNN)
- **K**: 5 (n√∫mero de vecinos)
- **Distancia**: Euclidiana (L2)
- **Votaci√≥n**: Mayor√≠a simple
- **Features**: Top 10 features m√°s importantes (reducci√≥n de dimensionalidad)
- **Dataset de Prueba**: 1,000 observaciones (balanceadas por clase)

---""")
    
    # TAREA 21: CODE
    add_cell(cells, "code", """# ============================================
# TAREA 21: C√≥digo de Selecci√≥n y An√°lisis
# ============================================

# Este c√≥digo documenta la selecci√≥n del algoritmo y genera un reporte

print("="*80)
print("TAREA 21: SELECCI√ìN DE ALGORITMO PARA IMPLEMENTACI√ìN EN C")
print("="*80)
print("\\nüéØ ALGORITMO SELECCIONADO: K-Nearest Neighbors (KNN)\\n")

# An√°lisis de complejidad
algorithms_analysis = {
    'KNN': {
        'Complejidad Entrenamiento': 'O(1)',
        'Complejidad Predicci√≥n': 'O(n*d)',
        'Simplicidad Implementaci√≥n': 'Alta',
        'Estructuras de Datos': 'Arrays simples',
        'Matem√°ticas Requeridas': 'B√°sicas',
        'Puntuaci√≥n Implementabilidad': 9.5
    },
    'Logistic Regression': {
        'Complejidad Entrenamiento': 'O(n*d*iter)',
        'Complejidad Predicci√≥n': 'O(d)',
        'Simplicidad Implementaci√≥n': 'Media',
        'Estructuras de Datos': 'Arrays + matrices',
        'Matem√°ticas Requeridas': 'Avanzadas',
        'Puntuaci√≥n Implementabilidad': 6.0
    },
    'Decision Tree': {
        'Complejidad Entrenamiento': 'O(n*d*log(n))',
        'Complejidad Predicci√≥n': 'O(log(n))',
        'Simplicidad Implementaci√≥n': 'Baja',
        'Estructuras de Datos': '√Årboles recursivos',
        'Matem√°ticas Requeridas': 'Medias',
        'Puntuaci√≥n Implementabilidad': 5.0
    },
    'Naive Bayes': {
        'Complejidad Entrenamiento': 'O(n*d)',
        'Complejidad Predicci√≥n': 'O(d*c)',
        'Simplicidad Implementaci√≥n': 'Media',
        'Estructuras de Datos': 'Arrays + probabilidades',
        'Matem√°ticas Requeridas': 'Medias-Avanzadas',
        'Puntuaci√≥n Implementabilidad': 6.5
    },
    'Perceptron': {
        'Complejidad Entrenamiento': 'O(n*d*iter)',
        'Complejidad Predicci√≥n': 'O(d)',
        'Simplicidad Implementaci√≥n': 'Media-Alta',
        'Estructuras de Datos': 'Arrays',
        'Matem√°ticas Requeridas': 'B√°sicas-Medias',
        'Puntuaci√≥n Implementabilidad': 7.5
    }
}

# Mostrar tabla comparativa
df_comparison = pd.DataFrame(algorithms_analysis).T
print("\\nüìä TABLA COMPARATIVA DE ALGORITMOS:\\n")
print(df_comparison.to_string())

# Visualizar puntuaciones
fig, ax = plt.subplots(figsize=(10, 6))
scores = [alg['Puntuaci√≥n Implementabilidad'] for alg in algorithms_analysis.values()]
names = list(algorithms_analysis.keys())
colors = ['green' if name == 'KNN' else 'skyblue' for name in names]

bars = ax.barh(names, scores, color=colors, edgecolor='black')
ax.set_xlabel('Puntuaci√≥n de Implementabilidad (0-10)', fontsize=12, fontweight='bold')
ax.set_title('Comparaci√≥n de Algoritmos para Implementaci√≥n en C', 
             fontsize=14, fontweight='bold', pad=20)
ax.set_xlim(0, 10)
ax.axvline(x=7, color='red', linestyle='--', alpha=0.5, label='Umbral Recomendado')
ax.legend()
ax.grid(axis='x', alpha=0.3)

# A√±adir valores en las barras
for bar in bars:
    width = bar.get_width()
    ax.text(width + 0.1, bar.get_y() + bar.get_height()/2,
            f'{width:.1f}',
            ha='left', va='center', fontweight='bold')

plt.tight_layout()
plt.savefig('tarea21_algorithm_selection.png', dpi=300, bbox_inches='tight')
plt.show()

print("\\n‚úÖ Justificaci√≥n guardada en: tarea21_algorithm_selection.png")

# Guardar justificaci√≥n en archivo de texto
justification_text = f\"\"\"
================================================================================
TAREA 21: SELECCI√ìN Y JUSTIFICACI√ìN DE ALGORITMO
================================================================================

ALGORITMO SELECCIONADO: K-Nearest Neighbors (KNN)

CRITERIOS DE SELECCI√ìN:
1. Simplicidad de implementaci√≥n: Alta
2. Complejidad de entrenamiento: M√≠nima (O(1))
3. Estructuras de datos requeridas: Simples (arrays)
4. Matem√°ticas requeridas: B√°sicas (distancia euclidiana)
5. Facilidad de debugging: Alta
6. Tiempo de desarrollo estimado: Bajo

CONFIGURACI√ìN:
- K (vecinos): 5
- M√©trica de distancia: Euclidiana (L2)
- Estrategia de votaci√≥n: Mayor√≠a simple
- Features: Top 10 m√°s importantes
- Dataset: 1,000 observaciones balanceadas

VENTAJAS:
+ No requiere fase de entrenamiento compleja
+ Implementaci√≥n directa sin optimizaciones complejas
+ F√°cil validaci√≥n paso a paso
+ Comparaci√≥n directa con sklearn

DESVENTAJAS ACEPTADAS:
- Complejidad O(n*d) en predicci√≥n
- Sensible a escalamiento (se resolver√° con normalizaci√≥n)
- Uso de memoria (se mitiga con dataset reducido)

ALTERNATIVAS DESCARTADAS Y RAZONES:
- Logistic Regression: Entrenamiento con gradiente descendente complejo
- Decision Tree: Algoritmo de construcci√≥n y estructuras recursivas complejas
- Naive Bayes: Estimaci√≥n de probabilidades y manejo de underflow
- Perceptron: Limitado a problemas linealmente separables

CONCLUSI√ìN:
KNN es la opci√≥n √≥ptima para demostrar comprensi√≥n algor√≠tmica profunda
mediante implementaci√≥n en C, balanceando simplicidad, efectividad y
valor educativo.

Puntuaci√≥n de Implementabilidad: 9.5/10
================================================================================
\"\"\"

with open('tarea21_justificacion_algoritmo.txt', 'w', encoding='utf-8') as f:
    f.write(justification_text)

print("\\n‚úÖ Justificaci√≥n completa guardada en: tarea21_justificacion_algoritmo.txt")
print("\\n" + "="*80)
print("TAREA 21 COMPLETADA ‚úÖ")
print("="*80)""")
    
    # Continue with TAREA 22...
    # Due to length, I'll add it in parts
    
    # Save notebook
    notebook["cells"] = cells
    with open('notebooks/seccion6.ipynb', 'w', encoding='utf-8') as f:
        json.dump(notebook, f, indent=1, ensure_ascii=False)
    
    print(f"‚úÖ Notebook updated with {len(cells)} cells")
    print("‚úÖ Tarea 21 added successfully!")

if __name__ == "__main__":
    create_complete_section6()
