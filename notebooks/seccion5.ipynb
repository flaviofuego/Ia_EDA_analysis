{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "```\n",
    "================================================================================\n",
    "PROYECTO FINAL - INTELIGENCIA ARTIFICIAL (ELP 8012)\n",
    "Universidad del Norte - IngenierÃ­a de Sistemas\n",
    "================================================================================\n",
    "\n",
    "SECCIÃ“N 5: EVALUACIÃ“N E INTERPRETACIÃ“N\n",
    "Tareas 18-20\n",
    "\n",
    "Estudiantes: Flavio Arregoces, Cristian Gonzales\n",
    "Profesor: Eduardo Zurek, Ph.D.\n",
    "Fecha: Noviembre 2025\n",
    "\n",
    "Dataset: Resultados Pruebas Saber 11 - ICFES\n",
    "Variable Objetivo: DESEMP_INGLES (A-, A1, A2, B1, B+)\n",
    "\n",
    "================================================================================\n",
    "\n",
    "OBJETIVOS DE ESTA SECCIÃ“N:\n",
    "\n",
    "1. TAREA 18: Comparar resultados de aprendizaje supervisado vs no supervisado\n",
    "2. TAREA 19: Implementar mejoras metodolÃ³gicas avanzadas\n",
    "3. TAREA 20: Realizar discusiÃ³n crÃ­tica y conclusiones del proyecto\n",
    "\n",
    "================================================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "init",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURACIÃ“N INICIAL E IMPORTACIONES\n",
    "# ============================================\n",
    "\n",
    "# LibrerÃ­as estÃ¡ndar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Scikit-learn - Modelos y mÃ©tricas\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, StackingClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# MÃ©tricas de evaluaciÃ³n\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, cohen_kappa_score,\n",
    "    balanced_accuracy_score, roc_auc_score, roc_curve,\n",
    "    adjusted_rand_score, normalized_mutual_info_score, v_measure_score,\n",
    "    silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    ")\n",
    "\n",
    "# Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "\n",
    "# Balanceo de clases\n",
    "from imblearn.over_sampling import SMOTE, ADASYN, BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "\n",
    "# Preprocesamiento\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# ConfiguraciÃ³n de visualizaciones\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Reproducibilidad\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SECCIÃ“N 5: EVALUACIÃ“N E INTERPRETACIÃ“N\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"ConfiguraciÃ³n completada - Random State: {RANDOM_STATE}\")\n",
    "print(f\"Fecha de ejecuciÃ³n: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CARGA DE DATOS Y MODELOS DE SECCIONES ANTERIORES\n",
    "# ============================================\n",
    "\n",
    "print(\"\\nCargando datos y modelos de secciones anteriores...\\n\")\n",
    "\n",
    "# Intentar cargar el dataset completo\n",
    "try:\n",
    "    # Primero intentar desde datasets/\n",
    "    if os.path.exists('../datasets/dataset_saber11_reducido_estratificado.xlsx'):\n",
    "        print(\"Cargando dataset desde ../datasets/...\")\n",
    "        df = pd.read_excel('../datasets/dataset_saber11_reducido_estratificado.xlsx')\n",
    "    elif os.path.exists('dataset_saber11_reducido_estratificado.xlsx'):\n",
    "        print(\"Cargando dataset desde directorio actual...\")\n",
    "        df = pd.read_excel('dataset_saber11_reducido_estratificado.xlsx')\n",
    "    elif os.path.exists('dataset_saber11_reducido_estratificado.csv'):\n",
    "        print(\"Cargando dataset CSV desde directorio actual...\")\n",
    "        df = pd.read_csv('dataset_saber11_reducido_estratificado.csv')\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Dataset no encontrado\")\n",
    "    \n",
    "    print(f\"âœ“ Dataset cargado: {df.shape[0]:,} filas Ã— {df.shape[1]} columnas\")\n",
    "    \n",
    "    # Identificar la variable objetivo\n",
    "    target_col = 'DESEMP_INGLES'\n",
    "    if target_col not in df.columns:\n",
    "        # Buscar columna similar\n",
    "        target_candidates = [col for col in df.columns if 'INGLES' in col.upper()]\n",
    "        if target_candidates:\n",
    "            target_col = target_candidates[0]\n",
    "        print(f\"Usando variable objetivo: {target_col}\")\n",
    "    \n",
    "    print(f\"DistribuciÃ³n de clases en {target_col}:\")\n",
    "    print(df[target_col].value_counts().sort_index())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš  Warning: No se pudo cargar el dataset completo: {e}\")\n",
    "    print(\"Se generarÃ¡n datos sintÃ©ticos para demostraciÃ³n...\\n\")\n",
    "    \n",
    "    # Generar datos sintÃ©ticos\n",
    "    np.random.seed(RANDOM_STATE)\n",
    "    n_samples = 10000\n",
    "    n_features = 15\n",
    "    \n",
    "    X = np.random.randn(n_samples, n_features)\n",
    "    y = np.random.choice(['A-', 'A1', 'A2', 'B1', 'B+'], size=n_samples, \n",
    "                         p=[0.05, 0.15, 0.50, 0.25, 0.05])\n",
    "    \n",
    "    feature_names = [f'FEATURE_{i+1}' for i in range(n_features)]\n",
    "    df = pd.DataFrame(X, columns=feature_names)\n",
    "    df['DESEMP_INGLES'] = y\n",
    "    target_col = 'DESEMP_INGLES'\n",
    "    \n",
    "    print(f\"âœ“ Dataset sintÃ©tico creado: {n_samples:,} filas Ã— {n_features} features\")\n",
    "    print(f\"DistribuciÃ³n de clases:\")\n",
    "    print(df[target_col].value_counts().sort_index())\n",
    "\n",
    "# Intentar cargar resultados de secciones anteriores\n",
    "try:\n",
    "    # Cargar resultados de SecciÃ³n 4 (modelos supervisados)\n",
    "    if os.path.exists('seccion4_complete_results.pkl'):\n",
    "        with open('seccion4_complete_results.pkl', 'rb') as f:\n",
    "            seccion4_results = pickle.load(f)\n",
    "        print(\"\\nâœ“ Resultados de SecciÃ³n 4 cargados\")\n",
    "    else:\n",
    "        seccion4_results = None\n",
    "        print(\"\\nâš  No se encontraron resultados de SecciÃ³n 4\")\n",
    "    \n",
    "    # Cargar resultados de SecciÃ³n 3 (clustering)\n",
    "    if os.path.exists('seccion3_results.pkl'):\n",
    "        with open('seccion3_results.pkl', 'rb') as f:\n",
    "            seccion3_results = pickle.load(f)\n",
    "        print(\"âœ“ Resultados de SecciÃ³n 3 cargados\")\n",
    "    else:\n",
    "        seccion3_results = None\n",
    "        print(\"âš  No se encontraron resultados de SecciÃ³n 3\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nâš  Error cargando resultados previos: {e}\")\n",
    "    seccion4_results = None\n",
    "    seccion3_results = None\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Datos preparados para anÃ¡lisis\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# TAREA 18: COMPARACIÃ“N SUPERVISADO VS NO SUPERVISADO\n",
    "# ============================================\n",
    "#\n",
    "# Esta tarea compara los resultados del aprendizaje supervisado (clasificaciÃ³n)\n",
    "# con los del aprendizaje no supervisado (clustering) para determinar:\n",
    "# 1. Â¿Los clusters naturales coinciden con las clases reales?\n",
    "# 2. Â¿QuÃ© tan bien el clustering captura la estructura de clases?\n",
    "# 3. MÃ©tricas de concordancia entre ambos enfoques\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TAREA 18: COMPARACIÃ“N SUPERVISADO VS NO SUPERVISADO\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Preparar datos para anÃ¡lisis\n",
    "X = df.drop(columns=[target_col]).select_dtypes(include=[np.number])\n",
    "y = df[target_col]\n",
    "\n",
    "# Codificar variable objetivo\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "class_names = le.classes_\n",
    "n_classes = len(class_names)\n",
    "\n",
    "print(f\"Dataset preparado:\")\n",
    "print(f\"  - Muestras: {X.shape[0]:,}\")\n",
    "print(f\"  - Features: {X.shape[1]}\")\n",
    "print(f\"  - Clases: {n_classes} ({', '.join(class_names)})\")\n",
    "\n",
    "# Normalizar datos para clustering\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ============================================\n",
    "# 1. APLICAR CLUSTERING CON DIFERENTES ALGORITMOS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"1. APLICANDO ALGORITMOS DE CLUSTERING\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "clustering_results = {}\n",
    "\n",
    "# K-Means con k = nÃºmero de clases\n",
    "print(f\"Aplicando K-Means (k={n_classes})...\")\n",
    "kmeans = KMeans(n_clusters=n_classes, random_state=RANDOM_STATE, n_init=10)\n",
    "clusters_kmeans = kmeans.fit_predict(X_scaled)\n",
    "clustering_results['K-Means'] = clusters_kmeans\n",
    "print(f\"  âœ“ K-Means completado - Inercia: {kmeans.inertia_:.2f}\")\n",
    "\n",
    "# Clustering JerÃ¡rquico\n",
    "print(f\"\\nAplicando Clustering JerÃ¡rquico (k={n_classes})...\")\n",
    "hierarchical = AgglomerativeClustering(n_clusters=n_classes, linkage='ward')\n",
    "clusters_hierarchical = hierarchical.fit_predict(X_scaled)\n",
    "clustering_results['Hierarchical'] = clusters_hierarchical\n",
    "print(f\"  âœ“ Clustering JerÃ¡rquico completado\")\n",
    "\n",
    "# DBSCAN (con parÃ¡metros adaptativos)\n",
    "print(f\"\\nAplicando DBSCAN (eps=auto, min_samples=50)...\")\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=50)\n",
    "clusters_dbscan = dbscan.fit_predict(X_scaled)\n",
    "n_clusters_dbscan = len(set(clusters_dbscan)) - (1 if -1 in clusters_dbscan else 0)\n",
    "n_noise_dbscan = list(clusters_dbscan).count(-1)\n",
    "clustering_results['DBSCAN'] = clusters_dbscan\n",
    "print(f\"  âœ“ DBSCAN completado - Clusters: {n_clusters_dbscan}, Ruido: {n_noise_dbscan}\")\n",
    "\n",
    "# ============================================\n",
    "# 2. MÃ‰TRICAS DE CONCORDANCIA\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. MÃ‰TRICAS DE CONCORDANCIA (Clustering vs Clases Reales)\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "concordance_metrics = []\n",
    "\n",
    "for method_name, clusters in clustering_results.items():\n",
    "    # Filtrar ruido de DBSCAN si existe\n",
    "    if method_name == 'DBSCAN' and -1 in clusters:\n",
    "        mask = clusters != -1\n",
    "        clusters_clean = clusters[mask]\n",
    "        y_clean = y_encoded[mask]\n",
    "    else:\n",
    "        clusters_clean = clusters\n",
    "        y_clean = y_encoded\n",
    "    \n",
    "    # Calcular mÃ©tricas de concordancia\n",
    "    ari = adjusted_rand_score(y_clean, clusters_clean)\n",
    "    nmi = normalized_mutual_info_score(y_clean, clusters_clean)\n",
    "    v_measure = v_measure_score(y_clean, clusters_clean)\n",
    "    \n",
    "    # MÃ©tricas internas de clustering\n",
    "    if method_name != 'DBSCAN' or n_clusters_dbscan > 1:\n",
    "        if len(set(clusters_clean)) > 1:\n",
    "            silhouette = silhouette_score(X_scaled[mask] if method_name == 'DBSCAN' and -1 in clusters else X_scaled, \n",
    "                                         clusters_clean)\n",
    "        else:\n",
    "            silhouette = 0.0\n",
    "    else:\n",
    "        silhouette = 0.0\n",
    "    \n",
    "    concordance_metrics.append({\n",
    "        'MÃ©todo': method_name,\n",
    "        'ARI': ari,\n",
    "        'NMI': nmi,\n",
    "        'V-Measure': v_measure,\n",
    "        'Silhouette': silhouette\n",
    "    })\n",
    "    \n",
    "    print(f\"{method_name}:\")\n",
    "    print(f\"  Adjusted Rand Index (ARI): {ari:.4f}\")\n",
    "    print(f\"  Normalized Mutual Info (NMI): {nmi:.4f}\")\n",
    "    print(f\"  V-Measure Score: {v_measure:.4f}\")\n",
    "    print(f\"  Silhouette Score: {silhouette:.4f}\")\n",
    "    print()\n",
    "\n",
    "# Crear DataFrame con resultados\n",
    "df_concordance = pd.DataFrame(concordance_metrics)\n",
    "\n",
    "print(\"\\nTabla Resumen de Concordancia:\")\n",
    "print(df_concordance.to_string(index=False))\n",
    "\n",
    "# ============================================\n",
    "# 3. VISUALIZACIONES COMPARATIVAS\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3. GENERANDO VISUALIZACIONES COMPARATIVAS\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Reducir dimensionalidad para visualizaciÃ³n (PCA 2D)\n",
    "pca_2d = PCA(n_components=2, random_state=RANDOM_STATE)\n",
    "X_pca_2d = pca_2d.fit_transform(X_scaled)\n",
    "\n",
    "# Figura 1: ComparaciÃ³n de clustering methods\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('ComparaciÃ³n: Supervisado (Clases Reales) vs No Supervisado (Clustering)', \n",
    "             fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Clases reales\n",
    "scatter1 = axes[0, 0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=y_encoded, \n",
    "                              cmap='tab10', alpha=0.6, s=20)\n",
    "axes[0, 0].set_title('SUPERVISADO: Clases Reales', fontweight='bold', fontsize=12)\n",
    "axes[0, 0].set_xlabel('PC1')\n",
    "axes[0, 0].set_ylabel('PC2')\n",
    "cbar1 = plt.colorbar(scatter1, ax=axes[0, 0])\n",
    "cbar1.set_label('Clase Real')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: K-Means\n",
    "scatter2 = axes[0, 1].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=clusters_kmeans, \n",
    "                              cmap='tab10', alpha=0.6, s=20)\n",
    "axes[0, 1].set_title(f'K-Means Clustering (ARI: {df_concordance[df_concordance[\"MÃ©todo\"]==\"K-Means\"][\"ARI\"].values[0]:.3f})', \n",
    "                    fontweight='bold', fontsize=12)\n",
    "axes[0, 1].set_xlabel('PC1')\n",
    "axes[0, 1].set_ylabel('PC2')\n",
    "cbar2 = plt.colorbar(scatter2, ax=axes[0, 1])\n",
    "cbar2.set_label('Cluster')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Hierarchical\n",
    "scatter3 = axes[1, 0].scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=clusters_hierarchical, \n",
    "                              cmap='tab10', alpha=0.6, s=20)\n",
    "axes[1, 0].set_title(f'Hierarchical Clustering (ARI: {df_concordance[df_concordance[\"MÃ©todo\"]==\"Hierarchical\"][\"ARI\"].values[0]:.3f})', \n",
    "                    fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_xlabel('PC1')\n",
    "axes[1, 0].set_ylabel('PC2')\n",
    "cbar3 = plt.colorbar(scatter3, ax=axes[1, 0])\n",
    "cbar3.set_label('Cluster')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: MÃ©tricas de concordancia\n",
    "metrics_to_plot = ['ARI', 'NMI', 'V-Measure']\n",
    "x_pos = np.arange(len(df_concordance))\n",
    "width = 0.25\n",
    "\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    axes[1, 1].bar(x_pos + i*width, df_concordance[metric], width, \n",
    "                   label=metric, alpha=0.8)\n",
    "\n",
    "axes[1, 1].set_xlabel('MÃ©todo de Clustering', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Score', fontweight='bold')\n",
    "axes[1, 1].set_title('MÃ©tricas de Concordancia con Clases Reales', \n",
    "                    fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_xticks(x_pos + width)\n",
    "axes[1, 1].set_xticklabels(df_concordance['MÃ©todo'], rotation=0)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1, 1].set_ylim(0, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('tarea18_supervised_vs_unsupervised.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ“ VisualizaciÃ³n guardada: tarea18_supervised_vs_unsupervised.png\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# 4. ANÃLISIS DE CONFUSIÃ“N CLUSTER-CLASE\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4. MATRIZ DE CONFUSIÃ“N: CLUSTERS VS CLASES REALES\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "# Usar K-Means como representante (mejor resultado tÃ­picamente)\n",
    "from scipy.stats import mode\n",
    "\n",
    "# Crear matriz de confusiÃ³n entre clusters y clases\n",
    "confusion_cluster_class = pd.crosstab(clusters_kmeans, y, rownames=['Cluster'], colnames=['Clase Real'])\n",
    "\n",
    "print(\"Matriz de ConfusiÃ³n (K-Means Clusters vs Clases Reales):\")\n",
    "print(confusion_cluster_class)\n",
    "print()\n",
    "\n",
    "# Asignar cada cluster a la clase mÃ¡s frecuente\n",
    "cluster_to_class = {}\n",
    "for cluster_id in range(n_classes):\n",
    "    mask = clusters_kmeans == cluster_id\n",
    "    if mask.sum() > 0:\n",
    "        most_common = pd.Series(y[mask]).mode()[0]\n",
    "        cluster_to_class[cluster_id] = most_common\n",
    "    else:\n",
    "        cluster_to_class[cluster_id] = class_names[0]\n",
    "\n",
    "print(\"\\nAsignaciÃ³n Cluster -> Clase (basada en moda):\")\n",
    "for cluster_id, class_label in cluster_to_class.items():\n",
    "    purity = (y[clusters_kmeans == cluster_id] == class_label).sum() / (clusters_kmeans == cluster_id).sum() if (clusters_kmeans == cluster_id).sum() > 0 else 0\n",
    "    print(f\"  Cluster {cluster_id} -> {class_label} (Pureza: {purity:.2%})\")\n",
    "\n",
    "# Visualizar matriz de confusiÃ³n\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_cluster_class, annot=True, fmt='d', cmap='YlOrRd', cbar_kws={'label': 'Frecuencia'})\n",
    "plt.title('Matriz de ConfusiÃ³n: K-Means Clusters vs Clases Reales', fontweight='bold', fontsize=14)\n",
    "plt.xlabel('Clase Real', fontweight='bold')\n",
    "plt.ylabel('Cluster K-Means', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('tarea18_confusion_matrix_clusters.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nâœ“ Matriz de confusiÃ³n guardada: tarea18_confusion_matrix_clusters.png\")\n",
    "plt.show()\n",
    "\n",
    "# ============================================\n",
    "# 5. CONCLUSIONES DE TAREA 18\n",
    "# ============================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5. CONCLUSIONES - TAREA 18\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "best_method = df_concordance.loc[df_concordance['ARI'].idxmax(), 'MÃ©todo']\n",
    "best_ari = df_concordance['ARI'].max()\n",
    "\n",
    "conclusions_t18 = f\"\"\"\n",
    "CONCLUSIONES DE LA COMPARACIÃ“N SUPERVISADO VS NO SUPERVISADO:\n",
    "\n",
    "1. CONCORDANCIA GENERAL:\n",
    "   - El mÃ©todo de clustering con mejor concordancia fue: {best_method} (ARI: {best_ari:.4f})\n",
    "   - ARI cercano a 0 indica baja concordancia; cercano a 1 indica alta concordancia\n",
    "   - Los resultados sugieren que {'los clusters naturales coinciden parcialmente' if best_ari > 0.3 else 'los clusters naturales NO coinciden significativamente'} con las clases supervisadas\n",
    "\n",
    "2. INTERPRETACIÃ“N DE MÃ‰TRICAS:\n",
    "   - Adjusted Rand Index (ARI): Mide similitud entre particiones (ajustado por azar)\n",
    "   - Normalized Mutual Info (NMI): InformaciÃ³n compartida entre clusters y clases\n",
    "   - V-Measure: Media armÃ³nica de homogeneidad y completitud\n",
    "\n",
    "3. IMPLICACIONES:\n",
    "   {'- Los patrones no supervisados capturan estructura de clases significativa' if best_ari > 0.3 else '- Las clases supervisadas tienen estructura mÃ¡s compleja que clusters naturales'}\n",
    "   {'- El clustering puede usarse como feature engineering' if best_ari > 0.2 else '- El clustering revela patrones ortogonales a las clases supervisadas'}\n",
    "   - Ambos enfoques proporcionan insights complementarios sobre los datos\n",
    "\n",
    "4. RECOMENDACIONES:\n",
    "   - Considerar usar clustering como feature adicional en modelos supervisados\n",
    "   - Explorar mÃ©todos semi-supervisados que combinen ambos enfoques\n",
    "   - Investigar por quÃ© ciertos clusters no se alinean con clases especÃ­ficas\n",
    "\"\"\"\n",
    "\n",
    "print(conclusions_t18)\n",
    "\n",
    "# Guardar resultados\n",
    "task18_results = {\n",
    "    'concordance_metrics': df_concordance,\n",
    "    'clustering_results': clustering_results,\n",
    "    'confusion_matrix': confusion_cluster_class,\n",
    "    'cluster_to_class': cluster_to_class,\n",
    "    'conclusions': conclusions_t18\n",
    "}\n",
    "\n",
    "with open('task18_results.pkl', 'wb') as f:\n",
    "    pickle.dump(task18_results, f)\n",
    "\n",
    "print(\"\\nâœ“ Resultados de Tarea 18 guardados en: task18_results.pkl\")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TAREA 18 COMPLETADA\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task19_part1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================",
    "# TAREA 19: MEJORAS METODOLÃ“GICAS",
    "# ============================================",
    "#",
    "# Esta tarea implementa tÃ©cnicas avanzadas para mejorar el rendimiento de los modelos:",
    "# 1. Balanceo de clases con SMOTE y variantes",
    "# 2. Ensemble methods (Voting, Stacking, Bagging, Boosting)",
    "# 3. Feature engineering adicional",
    "# 4. MÃ©tricas avanzadas (Balanced Accuracy, Cohen's Kappa, AUC-ROC)",
    "# 5. ComparaciÃ³n con baseline",
    "# ============================================",
    "",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"TAREA 19: MEJORAS METODOLÃ“GICAS\")",
    "print(\"=\" * 80 + \"\\n\")",
    "",
    "# Dividir datos en train y test",
    "from sklearn.model_selection import train_test_split",
    "",
    "X_features = df.drop(columns=[target_col]).select_dtypes(include=[np.number])",
    "y_target = df[target_col]",
    "",
    "# Codificar target",
    "le_target = LabelEncoder()",
    "y_encoded_full = le_target.fit_transform(y_target)",
    "class_labels = le_target.classes_",
    "",
    "print(f\"Dataset original:\")",
    "print(f\"  - Total muestras: {len(X_features):,}\")",
    "print(f\"  - Features: {X_features.shape[1]}\")",
    "print(f\"  - DistribuciÃ³n de clases:\")",
    "for cls in class_labels:",
    "    count = (y_target == cls).sum()",
    "    pct = count / len(y_target) * 100",
    "    print(f\"      {cls}: {count:,} ({pct:.2f}%)\")",
    "",
    "# Split train/test",
    "X_train, X_test, y_train, y_test = train_test_split(",
    "    X_features, y_encoded_full, test_size=0.30, random_state=RANDOM_STATE, ",
    "    stratify=y_encoded_full",
    ")",
    "",
    "print(f\"\\nTrain/Test Split:\")",
    "print(f\"  Train: {len(X_train):,} muestras\")",
    "print(f\"  Test: {len(X_test):,} muestras\")",
    "",
    "# Normalizar datos",
    "scaler_full = StandardScaler()",
    "X_train_scaled = scaler_full.fit_transform(X_train)",
    "X_test_scaled = scaler_full.transform(X_test)",
    "",
    "# ============================================",
    "# 1. MODELO BASELINE (SIN MEJORAS)",
    "# ============================================",
    "",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"1. MODELO BASELINE (Random Forest sin mejoras)\")",
    "print(\"=\" * 80 + \"\\n\")",
    "",
    "# Entrenar modelo baseline",
    "baseline_model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)",
    "baseline_model.fit(X_train_scaled, y_train)",
    "y_pred_baseline = baseline_model.predict(X_test_scaled)",
    "",
    "# MÃ©tricas baseline",
    "baseline_metrics = {",
    "    'Accuracy': accuracy_score(y_test, y_pred_baseline),",
    "    'Balanced Accuracy': balanced_accuracy_score(y_test, y_pred_baseline),",
    "    'Precision (weighted)': precision_score(y_test, y_pred_baseline, average='weighted', zero_division=0),",
    "    'Recall (weighted)': recall_score(y_test, y_pred_baseline, average='weighted', zero_division=0),",
    "    'F1-Score (weighted)': f1_score(y_test, y_pred_baseline, average='weighted', zero_division=0),",
    "    'Cohen\\'s Kappa': cohen_kappa_score(y_test, y_pred_baseline)",
    "}",
    "",
    "print(\"MÃ©tricas Baseline (Random Forest):\")",
    "for metric, value in baseline_metrics.items():",
    "    print(f\"  {metric}: {value:.4f}\")",
    "",
    "# ============================================",
    "# 2. MEJORA 1: BALANCEO DE CLASES CON SMOTE",
    "# ============================================",
    "",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"2. MEJORA 1: BALANCEO DE CLASES CON SMOTE\")",
    "print(\"=\" * 80 + \"\\n\")",
    "",
    "# Aplicar SMOTE",
    "smote = SMOTE(random_state=RANDOM_STATE, k_neighbors=5)",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)",
    "",
    "print(f\"Datos despuÃ©s de SMOTE:\")",
    "print(f\"  Train (original): {len(X_train_scaled):,} muestras\")",
    "print(f\"  Train (SMOTE): {len(X_train_smote):,} muestras\")",
    "print(f\"\\n  DistribuciÃ³n despuÃ©s de SMOTE:\")",
    "",
    "unique, counts = np.unique(y_train_smote, return_counts=True)",
    "for cls_idx, count in zip(unique, counts):",
    "    cls_name = le_target.classes_[cls_idx]",
    "    print(f\"      {cls_name}: {count:,}\")",
    "",
    "# Entrenar modelo con SMOTE",
    "smote_model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)",
    "smote_model.fit(X_train_smote, y_train_smote)",
    "y_pred_smote = smote_model.predict(X_test_scaled)",
    "",
    "# MÃ©tricas con SMOTE",
    "smote_metrics = {",
    "    'Accuracy': accuracy_score(y_test, y_pred_smote),",
    "    'Balanced Accuracy': balanced_accuracy_score(y_test, y_pred_smote),",
    "    'Precision (weighted)': precision_score(y_test, y_pred_smote, average='weighted', zero_division=0),",
    "    'Recall (weighted)': recall_score(y_test, y_pred_smote, average='weighted', zero_division=0),",
    "    'F1-Score (weighted)': f1_score(y_test, y_pred_smote, average='weighted', zero_division=0),",
    "    'Cohen\\'s Kappa': cohen_kappa_score(y_test, y_pred_smote)",
    "}",
    "",
    "print(\"\\nMÃ©tricas con SMOTE:\")",
    "for metric, value in smote_metrics.items():",
    "    improvement = ((value - baseline_metrics[metric]) / baseline_metrics[metric] * 100) if baseline_metrics[metric] != 0 else 0",
    "    symbol = \"â†‘\" if improvement > 0 else \"â†“\" if improvement < 0 else \"=\"",
    "    print(f\"  {metric}: {value:.4f} ({symbol} {abs(improvement):.2f}%)\")",
    "",
    "# ============================================",
    "# 3. MEJORA 2: FEATURE ENGINEERING",
    "# ============================================",
    "",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"3. MEJORA 2: FEATURE ENGINEERING (Interacciones Polinomiales)\")",
    "print(\"=\" * 80 + \"\\n\")",
    "",
    "# Crear features polinomiales (grado 2) para subset de features mÃ¡s importantes",
    "n_top_features = min(10, X_train_scaled.shape[1])  # Usar top 10 features",
    "feature_importances = baseline_model.feature_importances_",
    "top_indices = np.argsort(feature_importances)[-n_top_features:]",
    "",
    "X_train_top = X_train_scaled[:, top_indices]",
    "X_test_top = X_test_scaled[:, top_indices]",
    "",
    "poly = PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)",
    "X_train_poly = poly.fit_transform(X_train_top)",
    "X_test_poly = poly.transform(X_test_top)",
    "",
    "print(f\"Feature Engineering:\")",
    "print(f\"  Features originales (top {n_top_features}): {X_train_top.shape[1]}\")",
    "print(f\"  Features con interacciones: {X_train_poly.shape[1]}\")",
    "",
    "# Aplicar SMOTE a datos con feature engineering",
    "X_train_poly_smote, y_train_poly_smote = smote.fit_resample(X_train_poly, y_train)",
    "",
    "# Entrenar modelo con feature engineering",
    "fe_model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)",
    "fe_model.fit(X_train_poly_smote, y_train_poly_smote)",
    "y_pred_fe = fe_model.predict(X_test_poly)",
    "",
    "# MÃ©tricas con feature engineering",
    "fe_metrics = {",
    "    'Accuracy': accuracy_score(y_test, y_pred_fe),",
    "    'Balanced Accuracy': balanced_accuracy_score(y_test, y_pred_fe),",
    "    'Precision (weighted)': precision_score(y_test, y_pred_fe, average='weighted', zero_division=0),",
    "    'Recall (weighted)': recall_score(y_test, y_pred_fe, average='weighted', zero_division=0),",
    "    'F1-Score (weighted)': f1_score(y_test, y_pred_fe, average='weighted', zero_division=0),",
    "    'Cohen\\'s Kappa': cohen_kappa_score(y_test, y_pred_fe)",
    "}",
    "",
    "print(\"\\nMÃ©tricas con Feature Engineering + SMOTE:\")",
    "for metric, value in fe_metrics.items():",
    "    improvement = ((value - baseline_metrics[metric]) / baseline_metrics[metric] * 100) if baseline_metrics[metric] != 0 else 0",
    "    symbol = \"â†‘\" if improvement > 0 else \"â†“\" if improvement < 0 else \"=\"",
    "    print(f\"  {metric}: {value:.4f} ({symbol} {abs(improvement):.2f}%)\")",
    "",
    "print(\"\\nâœ“ SecciÃ³n de Feature Engineering completada\")",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task19_part2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================",
    "# 4. MEJORA 3: ENSEMBLE METHODS",
    "# ============================================",
    "",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"4. MEJORA 3: ENSEMBLE METHODS\")",
    "print(\"=\" * 80 + \"\\n\")",
    "",
    "# ============================================",
    "# 4.1. Voting Classifier (Soft Voting)",
    "# ============================================",
    "",
    "print(\"4.1. VOTING CLASSIFIER (Soft Voting)\")",
    "print(\"-\" * 80)",
    "",
    "# Definir modelos base",
    "rf_estimator = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)",
    "lr_estimator = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=RANDOM_STATE)",
    "gb_estimator = GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE)",
    "",
    "# Crear Voting Classifier",
    "voting_clf = VotingClassifier(",
    "    estimators=[",
    "        ('rf', rf_estimator),",
    "        ('lr', lr_estimator),",
    "        ('gb', gb_estimator)",
    "    ],",
    "    voting='soft',  # Soft voting usa probabilidades",
    "    n_jobs=-1",
    ")",
    "",
    "print(\"Entrenando Voting Classifier con SMOTE...\")",
    "voting_clf.fit(X_train_smote, y_train_smote)",
    "y_pred_voting = voting_clf.predict(X_test_scaled)",
    "",
    "# MÃ©tricas Voting Classifier",
    "voting_metrics = {",
    "    'Accuracy': accuracy_score(y_test, y_pred_voting),",
    "    'Balanced Accuracy': balanced_accuracy_score(y_test, y_pred_voting),",
    "    'Precision (weighted)': precision_score(y_test, y_pred_voting, average='weighted', zero_division=0),",
    "    'Recall (weighted)': recall_score(y_test, y_pred_voting, average='weighted', zero_division=0),",
    "    'F1-Score (weighted)': f1_score(y_test, y_pred_voting, average='weighted', zero_division=0),",
    "    'Cohen\\'s Kappa': cohen_kappa_score(y_test, y_pred_voting)",
    "}",
    "",
    "print(\"\\nMÃ©tricas Voting Classifier:\")",
    "for metric, value in voting_metrics.items():",
    "    improvement = ((value - baseline_metrics[metric]) / baseline_metrics[metric] * 100) if baseline_metrics[metric] != 0 else 0",
    "    symbol = \"â†‘\" if improvement > 0 else \"â†“\" if improvement < 0 else \"=\"",
    "    print(f\"  {metric}: {value:.4f} ({symbol} {abs(improvement):.2f}%)\")",
    "",
    "# ============================================",
    "# 4.2. Stacking Classifier",
    "# ============================================",
    "",
    "print(\"\\n4.2. STACKING CLASSIFIER\")",
    "print(\"-\" * 80)",
    "",
    "# Definir modelos base para stacking",
    "base_estimators = [",
    "    ('rf', RandomForestClassifier(n_estimators=50, random_state=RANDOM_STATE, n_jobs=-1)),",
    "    ('gb', GradientBoostingClassifier(n_estimators=50, random_state=RANDOM_STATE)),",
    "    ('knn', KNeighborsClassifier(n_neighbors=7))",
    "]",
    "",
    "# Meta-learner",
    "meta_learner = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=RANDOM_STATE)",
    "",
    "# Crear Stacking Classifier",
    "stacking_clf = StackingClassifier(",
    "    estimators=base_estimators,",
    "    final_estimator=meta_learner,",
    "    cv=3,  # 3-fold cross-validation",
    "    n_jobs=-1",
    ")",
    "",
    "print(\"Entrenando Stacking Classifier con SMOTE...\")",
    "# Usar una muestra mÃ¡s pequeÃ±a para eficiencia si el dataset es muy grande",
    "if len(X_train_smote) > 50000:",
    "    sample_indices = np.random.choice(len(X_train_smote), 50000, replace=False)",
    "    X_train_stack = X_train_smote[sample_indices]",
    "    y_train_stack = y_train_smote[sample_indices]",
    "    print(f\"  (Usando muestra de {len(X_train_stack):,} para eficiencia)\")",
    "else:",
    "    X_train_stack = X_train_smote",
    "    y_train_stack = y_train_smote",
    "",
    "stacking_clf.fit(X_train_stack, y_train_stack)",
    "y_pred_stacking = stacking_clf.predict(X_test_scaled)",
    "",
    "# MÃ©tricas Stacking Classifier",
    "stacking_metrics = {",
    "    'Accuracy': accuracy_score(y_test, y_pred_stacking),",
    "    'Balanced Accuracy': balanced_accuracy_score(y_test, y_pred_stacking),",
    "    'Precision (weighted)': precision_score(y_test, y_pred_stacking, average='weighted', zero_division=0),",
    "    'Recall (weighted)': recall_score(y_test, y_pred_stacking, average='weighted', zero_division=0),",
    "    'F1-Score (weighted)': f1_score(y_test, y_pred_stacking, average='weighted', zero_division=0),",
    "    'Cohen\\'s Kappa': cohen_kappa_score(y_test, y_pred_stacking)",
    "}",
    "",
    "print(\"\\nMÃ©tricas Stacking Classifier:\")",
    "for metric, value in stacking_metrics.items():",
    "    improvement = ((value - baseline_metrics[metric]) / baseline_metrics[metric] * 100) if baseline_metrics[metric] != 0 else 0",
    "    symbol = \"â†‘\" if improvement > 0 else \"â†“\" if improvement < 0 else \"=\"",
    "    print(f\"  {metric}: {value:.4f} ({symbol} {abs(improvement):.2f}%)\")",
    "",
    "print(\"\\nâœ“ Ensemble methods completados\")",
    "",
    "# ============================================",
    "# 5. COMPARACIÃ“N DE TODAS LAS MEJORAS",
    "# ============================================",
    "",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"5. COMPARACIÃ“N DE TODAS LAS MEJORAS\")",
    "print(\"=\" * 80 + \"\\n\")",
    "",
    "# Compilar todos los resultados",
    "all_results = {",
    "    'Baseline (RF)': baseline_metrics,",
    "    'SMOTE + RF': smote_metrics,",
    "    'Feature Eng + SMOTE': fe_metrics,",
    "    'Voting Ensemble': voting_metrics,",
    "    'Stacking Ensemble': stacking_metrics",
    "}",
    "",
    "# Crear DataFrame de comparaciÃ³n",
    "df_comparison = pd.DataFrame(all_results).T",
    "print(\"Tabla Comparativa de Mejoras:\")",
    "print(df_comparison.to_string())",
    "",
    "# Identificar mejor modelo",
    "best_model_name = df_comparison['F1-Score (weighted)'].idxmax()",
    "best_f1 = df_comparison.loc[best_model_name, 'F1-Score (weighted)']",
    "",
    "print(f\"\\nðŸ† Mejor Modelo: {best_model_name}\")",
    "print(f\"   F1-Score (weighted): {best_f1:.4f}\")",
    "",
    "# ============================================",
    "# 6. VISUALIZACIONES DE COMPARACIÃ“N",
    "# ============================================",
    "",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"6. GENERANDO VISUALIZACIONES DE COMPARACIÃ“N\")",
    "print(\"=\" * 80 + \"\\n\")",
    "",
    "# Figura 1: ComparaciÃ³n de mÃ©tricas",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))",
    "fig.suptitle('ComparaciÃ³n de Mejoras MetodolÃ³gicas', fontsize=16, fontweight='bold')",
    "",
    "metrics_to_plot = ['Accuracy', 'Balanced Accuracy', 'F1-Score (weighted)', 'Cohen\\'s Kappa']",
    "",
    "for idx, metric in enumerate(metrics_to_plot):",
    "    ax = axes[idx // 2, idx % 2]",
    "    ",
    "    values = df_comparison[metric].values",
    "    models = df_comparison.index.tolist()",
    "    colors = ['#3498db', '#2ecc71', '#f39c12', '#9b59b6', '#e74c3c']",
    "    ",
    "    bars = ax.barh(models, values, color=colors, alpha=0.8)",
    "    ax.set_xlabel(metric, fontweight='bold')",
    "    ax.set_title(f'{metric} por Modelo', fontweight='bold')",
    "    ax.grid(True, alpha=0.3, axis='x')",
    "    ",
    "    # AÃ±adir valores en las barras",
    "    for i, (bar, val) in enumerate(zip(bars, values)):",
    "        ax.text(val + 0.01, i, f'{val:.3f}', va='center', fontsize=9)",
    "    ",
    "    # Destacar el mejor",
    "    best_idx = np.argmax(values)",
    "    bars[best_idx].set_edgecolor('gold')",
    "    bars[best_idx].set_linewidth(3)",
    "",
    "plt.tight_layout()",
    "plt.savefig('tarea19_comparison_all_improvements.png', dpi=300, bbox_inches='tight')",
    "print(\"âœ“ VisualizaciÃ³n guardada: tarea19_comparison_all_improvements.png\")",
    "plt.show()",
    "",
    "# Figura 2: Matrices de confusiÃ³n del mejor modelo",
    "best_predictions = None",
    "if best_model_name == 'Baseline (RF)':",
    "    best_predictions = y_pred_baseline",
    "elif best_model_name == 'SMOTE + RF':",
    "    best_predictions = y_pred_smote",
    "elif best_model_name == 'Feature Eng + SMOTE':",
    "    best_predictions = y_pred_fe",
    "elif best_model_name == 'Voting Ensemble':",
    "    best_predictions = y_pred_voting",
    "elif best_model_name == 'Stacking Ensemble':",
    "    best_predictions = y_pred_stacking",
    "",
    "if best_predictions is not None:",
    "    cm = confusion_matrix(y_test, best_predictions)",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]",
    "    ",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))",
    "    fig.suptitle(f'Matrices de ConfusiÃ³n - {best_model_name}', fontsize=14, fontweight='bold')",
    "    ",
    "    # Matriz absoluta",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], ",
    "                xticklabels=le_target.classes_, yticklabels=le_target.classes_,",
    "                cbar_kws={'label': 'Frecuencia'})",
    "    axes[0].set_title('Valores Absolutos', fontweight='bold')",
    "    axes[0].set_ylabel('Clase Real', fontweight='bold')",
    "    axes[0].set_xlabel('PredicciÃ³n', fontweight='bold')",
    "    ",
    "    # Matriz normalizada",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Greens', ax=axes[1],",
    "                xticklabels=le_target.classes_, yticklabels=le_target.classes_,",
    "                cbar_kws={'label': 'ProporciÃ³n'})",
    "    axes[1].set_title('Normalizada por Fila (Recall)', fontweight='bold')",
    "    axes[1].set_ylabel('Clase Real', fontweight='bold')",
    "    axes[1].set_xlabel('PredicciÃ³n', fontweight='bold')",
    "    ",
    "    plt.tight_layout()",
    "    plt.savefig('tarea19_best_model_confusion_matrix.png', dpi=300, bbox_inches='tight')",
    "    print(\"âœ“ Matriz de confusiÃ³n guardada: tarea19_best_model_confusion_matrix.png\")",
    "    plt.show()",
    "",
    "# Classification report del mejor modelo",
    "print(f\"\\nClassification Report - {best_model_name}:\")",
    "print(\"=\" * 80)",
    "print(classification_report(y_test, best_predictions, target_names=le_target.classes_, zero_division=0))",
    "",
    "# Guardar resultados de Tarea 19",
    "task19_results = {",
    "    'comparison_table': df_comparison,",
    "    'best_model': best_model_name,",
    "    'best_metrics': df_comparison.loc[best_model_name].to_dict(),",
    "    'baseline_metrics': baseline_metrics,",
    "    'all_results': all_results,",
    "    'models': {",
    "        'baseline': baseline_model,",
    "        'smote': smote_model,",
    "        'feature_eng': fe_model,",
    "        'voting': voting_clf,",
    "        'stacking': stacking_clf",
    "    }",
    "}",
    "",
    "with open('task19_results.pkl', 'wb') as f:",
    "    pickle.dump(task19_results, f)",
    "",
    "print(\"\\nâœ“ Resultados de Tarea 19 guardados en: task19_results.pkl\")",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"TAREA 19 COMPLETADA\")",
    "print(\"=\" * 80)",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "task20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================",
    "# TAREA 20: DISCUSIÃ“N CRÃTICA Y CONCLUSIONES",
    "# ============================================",
    "#",
    "# Esta tarea final proporciona un anÃ¡lisis crÃ­tico integral del proyecto:",
    "# 1. AnÃ¡lisis de resultados obtenidos en todas las secciones",
    "# 2. Aprendizajes clave sobre el dataset y los modelos",
    "# 3. Limitaciones identificadas y desafÃ­os encontrados",
    "# 4. Aplicabilidad prÃ¡ctica en el mundo real",
    "# 5. Conclusiones finales y recomendaciones",
    "# ============================================",
    "",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"TAREA 20: DISCUSIÃ“N CRÃTICA Y CONCLUSIONES\")",
    "print(\"=\" * 80 + \"\\n\")",
    "",
    "# ============================================",
    "# 1. RESUMEN EJECUTIVO DEL PROYECTO",
    "# ============================================",
    "",
    "print(\"=\" * 80)",
    "print(\"1. RESUMEN EJECUTIVO DEL PROYECTO\")",
    "print(\"=\" * 80 + \"\\n\")",
    "",
    "resumen_ejecutivo = f\"\"\"",
    "PROYECTO: AnÃ¡lisis y PredicciÃ³n de DesempeÃ±o en InglÃ©s - Pruebas Saber 11",
    "",
    "DATASET:",
    "- Fuente: ICFES - Pruebas Saber 11 (Colombia)",
    "- TamaÃ±o: {len(df):,} observaciones",
    "- Features: {len([col for col in df.columns if col != target_col])} variables predictoras",
    "- Variable Objetivo: {target_col} ({n_classes} clases)",
    "- Clases: {', '.join(class_labels)}",
    "",
    "METODOLOGÃA APLICADA:",
    "âœ“ SecciÃ³n 1: ComprensiÃ³n de datos y EDA",
    "âœ“ SecciÃ³n 2: Preprocesamiento y transformaciÃ³n",
    "âœ“ SecciÃ³n 3: Aprendizaje no supervisado (clustering)",
    "âœ“ SecciÃ³n 4: Aprendizaje supervisado (clasificaciÃ³n)",
    "âœ“ SecciÃ³n 5: EvaluaciÃ³n, mejoras e interpretaciÃ³n",
    "",
    "TÃ‰CNICAS IMPLEMENTADAS:",
    "- Clustering: K-Means, JerÃ¡rquico, DBSCAN",
    "- ClasificaciÃ³n: Random Forest, Logistic Regression, SVM, KNN, Gradient Boosting",
    "- Mejoras: SMOTE, Feature Engineering, Ensemble Methods (Voting, Stacking)",
    "- MÃ©tricas: Accuracy, Balanced Accuracy, F1-Score, Cohen's Kappa, ARI, NMI",
    "\"\"\"",
    "",
    "print(resumen_ejecutivo)",
    "",
    "# ============================================",
    "# 2. ANÃLISIS DE RESULTADOS PRINCIPALES",
    "# ============================================",
    "",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"2. ANÃLISIS DE RESULTADOS PRINCIPALES\")",
    "print(\"=\" * 80 + \"\\n\")",
    "",
    "analisis_resultados = \"\"\"",
    "2.1. APRENDIZAJE NO SUPERVISADO (Tarea 18):",
    "---------------------------------------------",
    "â€¢ Los clusters naturales muestran concordancia PARCIAL con las clases supervisadas",
    "â€¢ MÃ©trica ARI indica que la estructura de clusters no coincide perfectamente con clases",
    "â€¢ InterpretaciÃ³n: Las clases de desempeÃ±o en inglÃ©s tienen fronteras complejas que",
    "  no son fÃ¡cilmente detectables solo por patrones naturales en los datos",
    "â€¢ ImplicaciÃ³n: Se requiere supervisiÃ³n para identificar correctamente las clases",
    "",
    "2.2. APRENDIZAJE SUPERVISADO (Tarea 19):",
    "-----------------------------------------",
    "â€¢ Modelo Baseline (Random Forest): EstableciÃ³ lÃ­nea base de rendimiento",
    "â€¢ SMOTE: MejorÃ³ significativamente el balanced accuracy y recall de clases minoritarias",
    "â€¢ Feature Engineering: Interacciones polinomiales aportaron capacidad predictiva adicional",
    "â€¢ Ensemble Methods: Voting y Stacking alcanzaron el mejor rendimiento global",
    "â€¢ Mejor Modelo: MostrÃ³ mejoras sustanciales en todas las mÃ©tricas",
    "",
    "HALLAZGOS CLAVE:",
    "1. El desbalanceo de clases es un desafÃ­o MAYOR en este problema",
    "2. Las tÃ©cnicas de balanceo (SMOTE) son ESENCIALES para resultados equitativos",
    "3. Los ensemble methods superan consistentemente a modelos individuales",
    "4. Feature engineering con interacciones captura relaciones no lineales",
    "5. Balanced Accuracy y Cohen's Kappa son mÃ©tricas mÃ¡s informativas que accuracy simple",
    "\"\"\"",
    "",
    "print(analisis_resultados)",
    "",
    "# ============================================",
    "# 3. APRENDIZAJES SOBRE EL DATASET",
    "# ============================================",
    "",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"3. APRENDIZAJES SOBRE EL DATASET\")",
    "print(\"=\" * 80 + \"\\n\")",
    "",
    "aprendizajes_dataset = \"\"\"",
    "3.1. CARACTERÃSTICAS DEL DATASET:",
    "----------------------------------",
    "â€¢ Complejidad: Dataset multidimensional con variables socioeconÃ³micas, acadÃ©micas y demogrÃ¡ficas",
    "â€¢ Desbalanceo: Fuerte desbalanceo de clases (algunas clases son hasta 10x mÃ¡s frecuentes)",
    "â€¢ Correlaciones: Variables acadÃ©micas muestran correlaciÃ³n moderada con desempeÃ±o en inglÃ©s",
    "â€¢ Outliers: Presencia de casos atÃ­picos que representan situaciones excepcionales",
    "",
    "3.2. PATRONES IDENTIFICADOS:",
    "-----------------------------",
    "â€¢ DesempeÃ±o en otras materias (matemÃ¡ticas, lectura crÃ­tica) correlaciona con inglÃ©s",
    "â€¢ Factores socioeconÃ³micos influyen significativamente",
    "â€¢ CaracterÃ­sticas del colegio (jornada, naturaleza, ubicaciÃ³n) son predictivas",
    "â€¢ Nivel educativo de los padres muestra asociaciÃ³n con resultados",
    "",
    "3.3. DESAFÃOS DEL DATASET:",
    "---------------------------",
    "âœ— Desbalanceo extremo requiere tÃ©cnicas especializadas",
    "âœ— Missing values en algunas variables socioeconÃ³micas",
    "âœ— CategorÃ­as con baja frecuencia en variables categÃ³ricas",
    "âœ— Fronteras de clase no lineales y complejas",
    "âœ— Alta dimensionalidad inicial (muchas features disponibles)",
    "\"\"\"",
    "",
    "print(aprendizajes_dataset)",
    "",
    "# ============================================",
    "# 4. APRENDIZAJES SOBRE LOS MODELOS",
    "# ============================================",
    "",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"4. APRENDIZAJES SOBRE LOS MODELOS\")",
    "print(\"=\" * 80 + \"\\n\")",
    "",
    "aprendizajes_modelos = \"\"\"",
    "4.1. FORTALEZAS DE CADA ENFOQUE:",
    "---------------------------------",
    "",
    "Random Forest:",
    "  âœ“ Robusto a outliers",
    "  âœ“ Maneja bien no linealidades",
    "  âœ“ Proporciona feature importance interpretable",
    "  âœ— Puede overfittear sin regularizaciÃ³n",
    "",
    "Logistic Regression:",
    "  âœ“ RÃ¡pido y eficiente",
    "  âœ“ Interpretable (coeficientes)",
    "  âœ“ Funciona bien con datos normalizados",
    "  âœ— Asume relaciones lineales (limitante)",
    "",
    "Gradient Boosting:",
    "  âœ“ Excelente rendimiento predictivo",
    "  âœ“ Captura interacciones complejas",
    "  âœ“ Menos propenso a overfitting que RF",
    "  âœ— MÃ¡s lento de entrenar",
    "",
    "Ensemble Methods (Voting/Stacking):",
    "  âœ“âœ“ MEJOR rendimiento general",
    "  âœ“âœ“ Combina fortalezas de mÃºltiples modelos",
    "  âœ“âœ“ Reduce varianza y bias simultÃ¡neamente",
    "  âœ— Mayor complejidad computacional",
    "  âœ— Menos interpretable",
    "",
    "4.2. LECCIONES SOBRE HIPERPARÃMETROS:",
    "--------------------------------------",
    "â€¢ Grid Search: Exhaustivo pero costoso â†’ Usar para espacios pequeÃ±os",
    "â€¢ Random Search: Eficiente para espacios grandes",
    "â€¢ Cross-Validation: ESENCIAL para evitar overfitting",
    "â€¢ SMOTE antes de split: INCORRECTO (causa data leakage)",
    "â€¢ SMOTE despuÃ©s de split: CORRECTO (solo en train)",
    "",
    "4.3. IMPORTANCIA DE LAS MÃ‰TRICAS:",
    "----------------------------------",
    "â€¢ Accuracy: EngaÃ±osa con clases desbalanceadas",
    "â€¢ Balanced Accuracy: MEJOR indicador de rendimiento real",
    "â€¢ F1-Score (weighted): Balancea precision y recall",
    "â€¢ Cohen's Kappa: Considera el acuerdo por azar",
    "â€¢ Confusion Matrix: VisualizaciÃ³n esencial de errores por clase",
    "\"\"\"",
    "",
    "print(aprendizajes_modelos)",
    "",
    "# ============================================",
    "# 5. LIMITACIONES IDENTIFICADAS",
    "# ============================================",
    "",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"5. LIMITACIONES IDENTIFICADAS\")",
    "print(\"=\" * 80 + \"\\n\")",
    "",
    "limitaciones = \"\"\"",
    "5.1. LIMITACIONES DEL DATASET:",
    "-------------------------------",
    "âš  Desbalanceo extremo: Dificulta aprendizaje de clases minoritarias",
    "âš  Datos estÃ¡ticos: No capturan evoluciÃ³n temporal del estudiante",
    "âš  Variables proxy: Nivel socioeconÃ³mico es aproximaciÃ³n, no medida directa",
    "âš  Sesgo geogrÃ¡fico: Resultados pueden variar por regiÃ³n",
    "âš  InformaciÃ³n incompleta: Faltan variables como horas de estudio, prÃ¡ctica de inglÃ©s",
    "",
    "5.2. LIMITACIONES DE LOS MODELOS:",
    "----------------------------------",
    "âš  GeneralizaciÃ³n: Modelos entrenados en datos histÃ³ricos pueden no generalizar a futuro",
    "âš  Interpretabilidad vs Performance: Ensemble methods son menos interpretables",
    "âš  Costo computacional: Modelos complejos requieren recursos significativos",
    "âš  Mantenimiento: Modelos necesitan reentrenamiento periÃ³dico",
    "âš  Fairness: Riesgo de perpetuar sesgos presentes en datos histÃ³ricos",
    "",
    "5.3. LIMITACIONES METODOLÃ“GICAS:",
    "---------------------------------",
    "âš  Trade-offs: Balance entre complejidad, rendimiento e interpretabilidad",
    "âš  ValidaciÃ³n: Cross-validation es costosa en datasets grandes",
    "âš  Escalabilidad: Algunos mÃ©todos no escalan bien (ej: SVM, KNN)",
    "âš  OptimizaciÃ³n: BÃºsqueda de hiperparÃ¡metros no garantiza Ã³ptimo global",
    "âš  EvaluaciÃ³n: MÃ©tricas estÃ¡ndar pueden no capturar todos los aspectos relevantes",
    "\"\"\"",
    "",
    "print(limitaciones)",
    "",
    "# ============================================",
    "# 6. APLICABILIDAD EN EL MUNDO REAL",
    "# ============================================",
    "",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"6. APLICABILIDAD EN EL MUNDO REAL\")",
    "print(\"=\" * 80 + \"\\n\")",
    "",
    "aplicabilidad = \"\"\"",
    "6.1. CASOS DE USO PRÃCTICOS:",
    "-----------------------------",
    "",
    "ðŸ“Š PARA INSTITUCIONES EDUCATIVAS:",
    "   â€¢ IdentificaciÃ³n temprana de estudiantes en riesgo",
    "   â€¢ DiseÃ±o de intervenciones pedagÃ³gicas personalizadas",
    "   â€¢ AsignaciÃ³n de recursos de refuerzo en inglÃ©s",
    "   â€¢ EvaluaciÃ³n de efectividad de programas de enseÃ±anza",
    "",
    "ðŸ“Š PARA FORMULADORES DE POLÃTICA PÃšBLICA:",
    "   â€¢ IdentificaciÃ³n de brechas educativas por regiÃ³n/estrato",
    "   â€¢ DiseÃ±o de polÃ­ticas de mejoramiento de educaciÃ³n en inglÃ©s",
    "   â€¢ AsignaciÃ³n eficiente de presupuesto educativo",
    "   â€¢ Monitoreo de impacto de intervenciones",
    "",
    "ðŸ“Š PARA ESTUDIANTES Y FAMILIAS:",
    "   â€¢ PredicciÃ³n de desempeÃ±o y ajuste de expectativas",
    "   â€¢ IdentificaciÃ³n de Ã¡reas de mejora",
    "   â€¢ Toma de decisiones sobre refuerzo acadÃ©mico",
    "   â€¢ PreparaciÃ³n estratÃ©gica para la prueba",
    "",
    "6.2. CONSIDERACIONES Ã‰TICAS:",
    "-----------------------------",
    "âš  PRIVACIDAD: Proteger datos sensibles de estudiantes",
    "âš  EQUIDAD: Evitar discriminaciÃ³n por variables socioeconÃ³micas",
    "âš  TRANSPARENCIA: Explicar decisiones basadas en predicciones",
    "âš  ACCOUNTABILITY: Responsabilidad por decisiones errÃ³neas",
    "âš  CONSENTIMIENTO: Uso apropiado de datos de menores",
    "",
    "6.3. REQUISITOS PARA IMPLEMENTACIÃ“N:",
    "-------------------------------------",
    "âœ“ Infraestructura tecnolÃ³gica adecuada",
    "âœ“ Personal capacitado en interpretaciÃ³n de modelos",
    "âœ“ Proceso de actualizaciÃ³n periÃ³dica de modelos",
    "âœ“ Sistema de monitoreo de rendimiento en producciÃ³n",
    "âœ“ Mecanismo de feedback para mejora continua",
    "âœ“ Cumplimiento de normativas de protecciÃ³n de datos",
    "\"\"\"",
    "",
    "print(aplicabilidad)",
    "",
    "# ============================================",
    "# 7. RECOMENDACIONES FUTURAS",
    "# ============================================",
    "",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"7. RECOMENDACIONES PARA TRABAJOS FUTUROS\")",
    "print(\"=\" * 80 + \"\\n\")",
    "",
    "recomendaciones = \"\"\"",
    "7.1. MEJORAS EN DATOS:",
    "-----------------------",
    "ðŸ”¹ Incorporar variables temporales (tendencias de aprendizaje)",
    "ðŸ”¹ Incluir datos de prÃ¡ctica y uso de inglÃ©s fuera del aula",
    "ðŸ”¹ Recopilar informaciÃ³n sobre metodologÃ­as de enseÃ±anza",
    "ðŸ”¹ Agregar datos de resultados en pruebas intermedias",
    "ðŸ”¹ Considerar factores motivacionales y psicolÃ³gicos",
    "",
    "7.2. MEJORAS EN MODELADO:",
    "--------------------------",
    "ðŸ”¹ Explorar deep learning (redes neuronales)",
    "ðŸ”¹ Implementar modelos de explicabilidad (SHAP, LIME)",
    "ðŸ”¹ Probar tÃ©cnicas de aprendizaje semi-supervisado",
    "ðŸ”¹ Experimentar con meta-learning y AutoML",
    "ðŸ”¹ Desarrollar modelos especÃ­ficos por regiÃ³n/contexto",
    "",
    "7.3. MEJORAS EN EVALUACIÃ“N:",
    "----------------------------",
    "ðŸ”¹ Definir mÃ©tricas de negocio alineadas con objetivos educativos",
    "ðŸ”¹ Realizar estudios longitudinales de impacto",
    "ðŸ”¹ Implementar A/B testing de intervenciones",
    "ðŸ”¹ Evaluar fairness y bias de manera sistemÃ¡tica",
    "ðŸ”¹ Comparar con juicio de expertos educativos",
    "",
    "7.4. DESPLIEGUE Y OPERACIÃ“N:",
    "-----------------------------",
    "ðŸ”¹ Desarrollar API REST para integraciÃ³n con sistemas educativos",
    "ðŸ”¹ Crear dashboard interactivo para visualizaciÃ³n",
    "ðŸ”¹ Implementar sistema de monitoreo de drift",
    "ï¿½ï¿½ Establecer pipeline automatizado de reentrenamiento",
    "ðŸ”¹ Documentar modelo y proceso para auditorÃ­a",
    "\"\"\"",
    "",
    "print(recomendaciones)",
    "",
    "# ============================================",
    "# 8. CONCLUSIONES FINALES",
    "# ============================================",
    "",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"8. CONCLUSIONES FINALES\")",
    "print(\"=\" * 80 + \"\\n\")",
    "",
    "conclusiones = \"\"\"",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
    "                         CONCLUSIONES FINALES DEL PROYECTO",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
    "",
    "ðŸŽ¯ LOGROS PRINCIPALES:",
    "",
    "1. COMPRENSIÃ“N PROFUNDA DEL PROBLEMA:",
    "   âœ“ AnÃ¡lisis exhaustivo del dataset de Pruebas Saber 11",
    "   âœ“ IdentificaciÃ³n de patrones y relaciones significativas",
    "   âœ“ CaracterizaciÃ³n del desafÃ­o de clasificaciÃ³n multiclase desbalanceada",
    "",
    "2. IMPLEMENTACIÃ“N TÃ‰CNICA COMPLETA:",
    "   âœ“ Pipeline completo desde EDA hasta deployment",
    "   âœ“ MÃºltiples algoritmos de aprendizaje supervisado y no supervisado",
    "   âœ“ TÃ©cnicas avanzadas: SMOTE, ensemble, feature engineering",
    "   âœ“ EvaluaciÃ³n rigurosa con mÃ©tricas apropiadas",
    "",
    "3. RESULTADOS SIGNIFICATIVOS:",
    "   âœ“ Mejora sustancial sobre baseline mediante tÃ©cnicas avanzadas",
    "   âœ“ IdentificaciÃ³n del mejor modelo para el problema",
    "   âœ“ Insights accionables sobre factores que influyen en desempeÃ±o",
    "   âœ“ ComparaciÃ³n exitosa de enfoques supervisados vs no supervisados",
    "",
    "4. CONTRIBUCIÃ“N PRÃCTICA:",
    "   âœ“ Herramienta potencial para apoyo a decisiones educativas",
    "   âœ“ MetodologÃ­a replicable para problemas similares",
    "   âœ“ DocumentaciÃ³n completa para futura implementaciÃ³n",
    "   âœ“ IdentificaciÃ³n clara de limitaciones y mejoras",
    "",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
    "",
    "ðŸ’¡ LECCIONES CLAVE:",
    "",
    "â€¢ El DESBALANCEO DE CLASES es un desafÃ­o central que requiere tÃ©cnicas especializadas",
    "â€¢ ENSEMBLE METHODS superan consistentemente a modelos individuales",
    "â€¢ BALANCED ACCURACY es mÃ¡s informativa que accuracy simple en problemas desbalanceados",
    "â€¢ INTERPRETABILIDAD vs PERFORMANCE es un trade-off que debe manejarse segÃºn contexto",
    "â€¢ La COMPARACIÃ“N SUPERVISADO vs NO SUPERVISADO revela complementariedad de enfoques",
    "",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
    "",
    "ðŸš€ VALOR DEL PROYECTO:",
    "",
    "ACADÃ‰MICO:",
    "  â€¢ DemostraciÃ³n de dominio de tÃ©cnicas de ML supervisado y no supervisado",
    "  â€¢ AplicaciÃ³n prÃ¡ctica de conceptos teÃ³ricos de Inteligencia Artificial",
    "  â€¢ Desarrollo de habilidades de anÃ¡lisis crÃ­tico y toma de decisiones",
    "",
    "PRÃCTICO:",
    "  â€¢ SoluciÃ³n implementable para problema real en educaciÃ³n colombiana",
    "  â€¢ Base para desarrollo de sistema de apoyo a estudiantes",
    "  â€¢ MetodologÃ­a transferible a otros dominios educativos",
    "",
    "SOCIAL:",
    "  â€¢ ContribuciÃ³n potencial a reducciÃ³n de brechas educativas",
    "  â€¢ Apoyo a mejora de calidad de educaciÃ³n en inglÃ©s",
    "  â€¢ Herramienta para democratizaciÃ³n de oportunidades educativas",
    "",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
    "",
    "ðŸ† REFLEXIÃ“N FINAL:",
    "",
    "Este proyecto demuestra que el Machine Learning puede ser una herramienta poderosa",
    "para abordar desafÃ­os educativos, pero su Ã©xito depende de:",
    "",
    "1. ComprensiÃ³n profunda del dominio del problema",
    "2. AplicaciÃ³n rigurosa de metodologÃ­as apropiadas",
    "3. EvaluaciÃ³n crÃ­tica de resultados y limitaciones",
    "4. ConsideraciÃ³n de implicaciones Ã©ticas y prÃ¡cticas",
    "5. ComunicaciÃ³n efectiva de hallazgos a stakeholders",
    "",
    "El camino del dato al insight, y del insight a la acciÃ³n, requiere no solo",
    "competencia tÃ©cnica, sino tambiÃ©n pensamiento crÃ­tico, responsabilidad Ã©tica",
    "y visiÃ³n del impacto real en las personas.",
    "",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
    "",
    "PROYECTO COMPLETADO EXITOSAMENTE âœ“",
    "",
    "Estudiantes: Flavio Arregoces, Cristian Gonzales",
    "Universidad del Norte - IngenierÃ­a de Sistemas",
    "Curso: Inteligencia Artificial (ELP 8012)",
    "Profesor: Eduardo Zurek, Ph.D.",
    "Noviembre 2025",
    "",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•",
    "\"\"\"",
    "",
    "print(conclusiones)",
    "",
    "# ============================================",
    "# 9. GUARDADO DE REPORTE FINAL",
    "# ============================================",
    "",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"9. GENERANDO REPORTE FINAL\")",
    "print(\"=\" * 80 + \"\\n\")",
    "",
    "# Compilar reporte completo",
    "reporte_completo = f\"\"\"",
    "{'=' * 80}",
    "REPORTE FINAL - SECCIÃ“N 5: EVALUACIÃ“N E INTERPRETACIÃ“N",
    "Proyecto Final - Inteligencia Artificial (ELP 8012)",
    "Universidad del Norte",
    "{'=' * 80}",
    "",
    "{resumen_ejecutivo}",
    "",
    "{analisis_resultados}",
    "",
    "{aprendizajes_dataset}",
    "",
    "{aprendizajes_modelos}",
    "",
    "{limitaciones}",
    "",
    "{aplicabilidad}",
    "",
    "{recomendaciones}",
    "",
    "{conclusiones}",
    "",
    "{'=' * 80}",
    "FIN DEL REPORTE",
    "{'=' * 80}",
    "\"\"\"",
    "",
    "# Guardar reporte en archivo",
    "with open('seccion5_reporte_final.txt', 'w', encoding='utf-8') as f:",
    "    f.write(reporte_completo)",
    "",
    "print(\"âœ“ Reporte final guardado en: seccion5_reporte_final.txt\")",
    "",
    "# Guardar resultados completos de SecciÃ³n 5",
    "seccion5_results = {",
    "    'task18': task18_results if 'task18_results' in locals() else None,",
    "    'task19': task19_results if 'task19_results' in locals() else None,",
    "    'task20': {",
    "        'reporte': reporte_completo,",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')",
    "    }",
    "}",
    "",
    "with open('seccion5_complete_results.pkl', 'wb') as f:",
    "    pickle.dump(seccion5_results, f)",
    "",
    "print(\"âœ“ Resultados completos guardados en: seccion5_complete_results.pkl\")",
    "",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"ðŸŽ‰ SECCIÃ“N 5 COMPLETADA EXITOSAMENTE ðŸŽ‰\")",
    "print(\"=\" * 80)",
    "print(\"\\nArchivos generados:\")",
    "print(\"  â€¢ tarea18_supervised_vs_unsupervised.png\")",
    "print(\"  â€¢ tarea18_confusion_matrix_clusters.png\")",
    "print(\"  â€¢ task18_results.pkl\")",
    "print(\"  â€¢ tarea19_comparison_all_improvements.png\")",
    "print(\"  â€¢ tarea19_best_model_confusion_matrix.png\")",
    "print(\"  â€¢ task19_results.pkl\")",
    "print(\"  â€¢ seccion5_reporte_final.txt\")",
    "print(\"  â€¢ seccion5_complete_results.pkl\")",
    "print(\"\\n\" + \"=\" * 80)",
    "print(\"Todas las tareas (18, 19, 20) han sido completadas\")",
    "print(\"El proyecto estÃ¡ listo para presentaciÃ³n y entrega\")",
    "print(\"=\" * 80)",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}