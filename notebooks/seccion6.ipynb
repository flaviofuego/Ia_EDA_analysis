{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéì PROYECTO FINAL - INTELIGENCIA ARTIFICIAL\n",
    "## Secci√≥n 6: Implementaci√≥n en C\n",
    "\n",
    "---\n",
    "\n",
    "**Universidad del Norte** - Ingenier√≠a de Sistemas  \n",
    "**Curso**: Inteligencia Artificial (ELP 8012)  \n",
    "**Profesor**: Eduardo Zurek, Ph.D.  \n",
    "**Estudiantes**: Flavio Arregoces, Cristian Gonzales  \n",
    "**Fecha**: Noviembre 2025  \n",
    "\n",
    "---\n",
    "\n",
    "## üéØ OBJETIVOS DE ESTA SECCI√ìN\n",
    "\n",
    "Esta secci√≥n implementa un algoritmo de Machine Learning supervisado en lenguaje C para:\n",
    "1. Demostrar comprensi√≥n profunda del funcionamiento interno de los algoritmos\n",
    "2. Comparar implementaci√≥n manual vs bibliotecas de alto nivel\n",
    "3. Analizar ventajas y limitaciones de implementaciones en bajo nivel\n",
    "4. Evaluar trade-offs entre rendimiento, precisi√≥n y complejidad\n",
    "\n",
    "---\n",
    "\n",
    "## üìã CONTENIDO\n",
    "\n",
    "- **Tarea 21**: Selecci√≥n y justificaci√≥n del algoritmo a implementar\n",
    "- **Tarea 22**: Dise√±o de estructuras de datos y funciones (pseudoc√≥digo)\n",
    "- **Tarea 23**: Implementaci√≥n completa en C\n",
    "- **Tarea 24**: Evaluaci√≥n del desempe√±o y comparaci√≥n con Python\n",
    "- **Tarea 25**: An√°lisis de limitaciones y propuestas de optimizaci√≥n\n",
    "\n",
    "---\n",
    "\n",
    "**Variable Objetivo**: `DESEMP_INGLES` (5 clases: A-, A1, A2, B1, B+)  \n",
    "**Algoritmo Seleccionado**: K-Nearest Neighbors (KNN)  \n",
    "**Dataset**: Versi√≥n reducida estratificada de Pruebas Saber 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURACI√ìN INICIAL\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "import subprocess\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, balanced_accuracy_score\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n de visualizaci√≥n\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "# Configuraci√≥n de reproducibilidad\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"‚úÖ Librer√≠as importadas correctamente\")\n",
    "print(f\"üìä Random State: {RANDOM_STATE}\")\n",
    "print(f\"üìÅ Directorio de trabajo: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---",
    "",
    "# ============================================",
    "# TAREA 21: Selecci√≥n y Justificaci√≥n del Algoritmo",
    "# ============================================",
    "",
    "## üéØ Objetivo",
    "Seleccionar un algoritmo de aprendizaje supervisado para implementar en C y justificar t√©cnicamente la elecci√≥n.",
    "",
    "## üìä An√°lisis de Candidatos",
    "",
    "### Algoritmos Considerados:",
    "",
    "#### 1. K-Nearest Neighbors (KNN) ‚≠ê SELECCIONADO",
    "**Ventajas para implementaci√≥n en C:**",
    "- ‚úÖ Algoritmo conceptualmente simple (b√∫squeda + votaci√≥n)",
    "- ‚úÖ No requiere fase de entrenamiento compleja (solo almacenar datos)",
    "- ‚úÖ F√°cil de entender y debuggear",
    "- ‚úÖ Implementaci√≥n directa sin optimizaciones avanzadas",
    "- ‚úÖ Estructuras de datos simples (arrays)",
    "- ‚úÖ C√°lculos matem√°ticos b√°sicos (distancia euclidiana)",
    "",
    "**Desventajas:**",
    "- ‚ö†Ô∏è Complejidad O(n*d) en predicci√≥n (n=tama√±o dataset, d=dimensiones)",
    "- ‚ö†Ô∏è Sensible a escalamiento de features",
    "- ‚ö†Ô∏è Requiere mucha memoria para datasets grandes",
    "",
    "#### 2. Regresi√≥n Log√≠stica",
    "**Ventajas:**",
    "- ‚úÖ Interpretable",
    "- ‚úÖ R√°pida en predicci√≥n",
    "",
    "**Desventajas:**",
    "- ‚ùå Entrenamiento complejo (gradiente descendente, optimizaci√≥n)",
    "- ‚ùå Requiere manejo de convergencia",
    "- ‚ùå Multiclass (OvR o Softmax) a√±ade complejidad",
    "",
    "#### 3. √Årbol de Decisi√≥n",
    "**Ventajas:**",
    "- ‚úÖ Visualizable",
    "- ‚úÖ No requiere normalizaci√≥n",
    "",
    "**Desventajas:**",
    "- ‚ùå Algoritmo de construcci√≥n complejo (splits, gini, poda)",
    "- ‚ùå Estructuras de datos complejas (√°rboles, recursi√≥n)",
    "",
    "#### 4. Naive Bayes",
    "**Ventajas:**",
    "- ‚úÖ Simple probabil√≠sticamente",
    "",
    "**Desventajas:**",
    "- ‚ùå Requiere estimaci√≥n de distribuciones",
    "- ‚ùå Manejo de underflow num√©rico",
    "- ‚ùå Variables continuas requieren discretizaci√≥n",
    "",
    "#### 5. Perceptr√≥n",
    "**Ventajas:**",
    "- ‚úÖ Simple conceptualmente",
    "",
    "**Desventajas:**",
    "- ‚ùå Solo funciona bien para datos linealmente separables",
    "- ‚ùå Multiclass requiere estrategia adicional",
    "",
    "---",
    "",
    "## ‚úÖ DECISI√ìN FINAL: K-Nearest Neighbors (KNN)",
    "",
    "### Justificaci√≥n T√©cnica:",
    "",
    "1. **Simplicidad de Implementaci√≥n**: KNN no requiere entrenamiento complejo. Solo necesitamos:",
    "   - Almacenar datos de entrenamiento",
    "   - Calcular distancias euclidianas",
    "   - Encontrar k vecinos m√°s cercanos",
    "   - Votar por la clase mayoritaria",
    "",
    "2. **Estructuras de Datos Simples**: Se implementa con arrays est√°ticos en C, sin necesidad de √°rboles, grafos o estructuras din√°micas complejas.",
    "",
    "3. **Matem√°ticas Elementales**: Solo requiere:",
    "   - Ra√≠z cuadrada (disponible en math.h)",
    "   - Sumas y restas",
    "   - Comparaciones",
    "",
    "4. **Debugging Sencillo**: F√°cil de verificar paso a paso (imprimir distancias, vecinos, votos).",
    "",
    "5. **Rendimiento Aceptable**: Para un subconjunto reducido del dataset, KNN es viable y permite demostrar comprensi√≥n algor√≠tmica.",
    "",
    "6. **Comparaci√≥n Python vs C Significativa**: Podemos comparar directamente con sklearn.neighbors.KNeighborsClassifier",
    "",
    "---",
    "",
    "## üîß Configuraci√≥n Seleccionada",
    "",
    "- **Algoritmo**: K-Nearest Neighbors (KNN)",
    "- **K**: 5 (n√∫mero de vecinos)",
    "- **Distancia**: Euclidiana (L2)",
    "- **Votaci√≥n**: Mayor√≠a simple",
    "- **Features**: Top 10 features m√°s importantes (reducci√≥n de dimensionalidad)",
    "- **Dataset de Prueba**: 1,000 observaciones (balanceadas por clase)",
    "",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================",
    "# TAREA 21: C√≥digo de Selecci√≥n y An√°lisis",
    "# ============================================",
    "",
    "# Este c√≥digo documenta la selecci√≥n del algoritmo y genera un reporte",
    "",
    "print(\"=\"*80)",
    "print(\"TAREA 21: SELECCI√ìN DE ALGORITMO PARA IMPLEMENTACI√ìN EN C\")",
    "print(\"=\"*80)",
    "print(\"\\nüéØ ALGORITMO SELECCIONADO: K-Nearest Neighbors (KNN)\\n\")",
    "",
    "# An√°lisis de complejidad",
    "algorithms_analysis = {",
    "    'KNN': {",
    "        'Complejidad Entrenamiento': 'O(1)',",
    "        'Complejidad Predicci√≥n': 'O(n*d)',",
    "        'Simplicidad Implementaci√≥n': 'Alta',",
    "        'Estructuras de Datos': 'Arrays simples',",
    "        'Matem√°ticas Requeridas': 'B√°sicas',",
    "        'Puntuaci√≥n Implementabilidad': 9.5",
    "    },",
    "    'Logistic Regression': {",
    "        'Complejidad Entrenamiento': 'O(n*d*iter)',",
    "        'Complejidad Predicci√≥n': 'O(d)',",
    "        'Simplicidad Implementaci√≥n': 'Media',",
    "        'Estructuras de Datos': 'Arrays + matrices',",
    "        'Matem√°ticas Requeridas': 'Avanzadas',",
    "        'Puntuaci√≥n Implementabilidad': 6.0",
    "    },",
    "    'Decision Tree': {",
    "        'Complejidad Entrenamiento': 'O(n*d*log(n))',",
    "        'Complejidad Predicci√≥n': 'O(log(n))',",
    "        'Simplicidad Implementaci√≥n': 'Baja',",
    "        'Estructuras de Datos': '√Årboles recursivos',",
    "        'Matem√°ticas Requeridas': 'Medias',",
    "        'Puntuaci√≥n Implementabilidad': 5.0",
    "    },",
    "    'Naive Bayes': {",
    "        'Complejidad Entrenamiento': 'O(n*d)',",
    "        'Complejidad Predicci√≥n': 'O(d*c)',",
    "        'Simplicidad Implementaci√≥n': 'Media',",
    "        'Estructuras de Datos': 'Arrays + probabilidades',",
    "        'Matem√°ticas Requeridas': 'Medias-Avanzadas',",
    "        'Puntuaci√≥n Implementabilidad': 6.5",
    "    },",
    "    'Perceptron': {",
    "        'Complejidad Entrenamiento': 'O(n*d*iter)',",
    "        'Complejidad Predicci√≥n': 'O(d)',",
    "        'Simplicidad Implementaci√≥n': 'Media-Alta',",
    "        'Estructuras de Datos': 'Arrays',",
    "        'Matem√°ticas Requeridas': 'B√°sicas-Medias',",
    "        'Puntuaci√≥n Implementabilidad': 7.5",
    "    }",
    "}",
    "",
    "# Mostrar tabla comparativa",
    "df_comparison = pd.DataFrame(algorithms_analysis).T",
    "print(\"\\nüìä TABLA COMPARATIVA DE ALGORITMOS:\\n\")",
    "print(df_comparison.to_string())",
    "",
    "# Visualizar puntuaciones",
    "fig, ax = plt.subplots(figsize=(10, 6))",
    "scores = [alg['Puntuaci√≥n Implementabilidad'] for alg in algorithms_analysis.values()]",
    "names = list(algorithms_analysis.keys())",
    "colors = ['green' if name == 'KNN' else 'skyblue' for name in names]",
    "",
    "bars = ax.barh(names, scores, color=colors, edgecolor='black')",
    "ax.set_xlabel('Puntuaci√≥n de Implementabilidad (0-10)', fontsize=12, fontweight='bold')",
    "ax.set_title('Comparaci√≥n de Algoritmos para Implementaci√≥n en C', ",
    "             fontsize=14, fontweight='bold', pad=20)",
    "ax.set_xlim(0, 10)",
    "ax.axvline(x=7, color='red', linestyle='--', alpha=0.5, label='Umbral Recomendado')",
    "ax.legend()",
    "ax.grid(axis='x', alpha=0.3)",
    "",
    "# A√±adir valores en las barras",
    "for bar in bars:",
    "    width = bar.get_width()",
    "    ax.text(width + 0.1, bar.get_y() + bar.get_height()/2,",
    "            f'{width:.1f}',",
    "            ha='left', va='center', fontweight='bold')",
    "",
    "plt.tight_layout()",
    "plt.savefig('tarea21_algorithm_selection.png', dpi=300, bbox_inches='tight')",
    "plt.show()",
    "",
    "print(\"\\n‚úÖ Justificaci√≥n guardada en: tarea21_algorithm_selection.png\")",
    "",
    "# Guardar justificaci√≥n en archivo de texto",
    "justification_text = f\"\"\"",
    "================================================================================",
    "TAREA 21: SELECCI√ìN Y JUSTIFICACI√ìN DE ALGORITMO",
    "================================================================================",
    "",
    "ALGORITMO SELECCIONADO: K-Nearest Neighbors (KNN)",
    "",
    "CRITERIOS DE SELECCI√ìN:",
    "1. Simplicidad de implementaci√≥n: Alta",
    "2. Complejidad de entrenamiento: M√≠nima (O(1))",
    "3. Estructuras de datos requeridas: Simples (arrays)",
    "4. Matem√°ticas requeridas: B√°sicas (distancia euclidiana)",
    "5. Facilidad de debugging: Alta",
    "6. Tiempo de desarrollo estimado: Bajo",
    "",
    "CONFIGURACI√ìN:",
    "- K (vecinos): 5",
    "- M√©trica de distancia: Euclidiana (L2)",
    "- Estrategia de votaci√≥n: Mayor√≠a simple",
    "- Features: Top 10 m√°s importantes",
    "- Dataset: 1,000 observaciones balanceadas",
    "",
    "VENTAJAS:",
    "+ No requiere fase de entrenamiento compleja",
    "+ Implementaci√≥n directa sin optimizaciones complejas",
    "+ F√°cil validaci√≥n paso a paso",
    "+ Comparaci√≥n directa con sklearn",
    "",
    "DESVENTAJAS ACEPTADAS:",
    "- Complejidad O(n*d) en predicci√≥n",
    "- Sensible a escalamiento (se resolver√° con normalizaci√≥n)",
    "- Uso de memoria (se mitiga con dataset reducido)",
    "",
    "ALTERNATIVAS DESCARTADAS Y RAZONES:",
    "- Logistic Regression: Entrenamiento con gradiente descendente complejo",
    "- Decision Tree: Algoritmo de construcci√≥n y estructuras recursivas complejas",
    "- Naive Bayes: Estimaci√≥n de probabilidades y manejo de underflow",
    "- Perceptron: Limitado a problemas linealmente separables",
    "",
    "CONCLUSI√ìN:",
    "KNN es la opci√≥n √≥ptima para demostrar comprensi√≥n algor√≠tmica profunda",
    "mediante implementaci√≥n en C, balanceando simplicidad, efectividad y",
    "valor educativo.",
    "",
    "Puntuaci√≥n de Implementabilidad: 9.5/10",
    "================================================================================",
    "\"\"\"",
    "",
    "with open('tarea21_justificacion_algoritmo.txt', 'w', encoding='utf-8') as f:",
    "    f.write(justification_text)",
    "",
    "print(\"\\n‚úÖ Justificaci√≥n completa guardada en: tarea21_justificacion_algoritmo.txt\")",
    "print(\"\\n\" + \"=\"*80)",
    "print(\"TAREA 21 COMPLETADA ‚úÖ\")",
    "print(\"=\"*80)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}